{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AASHISH\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##Importing libraries\n",
    "import cv2\n",
    "import pickle\n",
    "import joblib \n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda,Bidirectional\n",
    "from keras.models import Model, Sequential, load_model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>lp</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./crop_m1/I00000.png</td>\n",
       "      <td>./crop_m1/I00000.png</td>\n",
       "      <td>9B52145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./crop_m1/I00000.png</td>\n",
       "      <td>./crop_h1/I00000.png</td>\n",
       "      <td>9B52145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./crop_m1/I00001.png</td>\n",
       "      <td>./crop_m1/I00001.png</td>\n",
       "      <td>6B94558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./crop_m1/I00001.png</td>\n",
       "      <td>./crop_h1/I00001.png</td>\n",
       "      <td>6B94558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./crop_m1/I00002.png</td>\n",
       "      <td>./crop_m1/I00002.png</td>\n",
       "      <td>8B90164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               track_id            image_path       lp  train\n",
       "0  ./crop_m1/I00000.png  ./crop_m1/I00000.png  9B52145      0\n",
       "1  ./crop_m1/I00000.png  ./crop_h1/I00000.png  9B52145      0\n",
       "2  ./crop_m1/I00001.png  ./crop_m1/I00001.png  6B94558      0\n",
       "3  ./crop_m1/I00001.png  ./crop_h1/I00001.png  6B94558      0\n",
       "4  ./crop_m1/I00002.png  ./crop_m1/I00002.png  8B90164      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"trainVal.csv\")  ##Reading CSV file\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From CSV file, taking track_id and lp columns for getting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Char_list contains the combination of alphabets and digits\n",
    "\n",
    "char_list = string.ascii_letters+string.digits\n",
    "\n",
    "# print(char_list) \n",
    "\n",
    "def encode_to_labels(txt):\n",
    "    # encoding each label into number\n",
    "    '''\n",
    "    for example : a:0,b:1,c:2 and word is \"abac\", then after passing from encode_to_label\n",
    "    it become [0102] . \n",
    "    \n",
    "    '''\n",
    "    dig_lst = []\n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            print(char)\n",
    "        \n",
    "    return dig_lst  ##output of dig_lst random example [58, 27, 61, 56, 57, 57, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DATA PREPROCESSING\n",
    "\n",
    "1. splitting the image and label into trainging and testing\n",
    "\n",
    "   There are total 652 images. After splitting, 521 training image and 131 testing image\n",
    "\n",
    "2. Resize the image to width = 128 and height = 32 so that every image is of fixed size and can pass into the model.\n",
    "\n",
    "'''\n",
    "# lists for training dataset\n",
    "training_img = [] \n",
    "training_txt = []\n",
    "train_input_length = []\n",
    "train_label_length = []\n",
    "orig_txt = []\n",
    " \n",
    "#lists for validation dataset\n",
    "valid_img = []\n",
    "valid_txt = []\n",
    "valid_input_length = []\n",
    "valid_label_length = []\n",
    "valid_orig_txt = []\n",
    "\n",
    "max_label_len = 0\n",
    "for imagepath in range(len(df)):\n",
    "    label = df['lp'][imagepath]\n",
    "#     print(imagepath)\n",
    "    \n",
    "    img = df['track_id'][imagepath][2:]\n",
    "    img = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY) \n",
    "    img = cv2.resize(img, (128,32))\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    if len(label) > max_label_len:\n",
    "        max_label_len = len(label)\n",
    "            \n",
    "\n",
    "    if imagepath%5 == 0:     \n",
    "        valid_orig_txt.append(label)   \n",
    "        valid_label_length.append(len(label)) #for CTC function\n",
    "        valid_input_length.append(31) #for CTC function\n",
    "        valid_img.append(img)\n",
    "        valid_txt.append(encode_to_labels(label))\n",
    "    else:\n",
    "        orig_txt.append(label)   \n",
    "        train_label_length.append(len(label)) #for CTC function\n",
    "        train_input_length.append(31) #for CTC function\n",
    "        training_img.append(img)\n",
    "        training_txt.append(encode_to_labels(label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding is used so that everyu label is of fixed size, to prevent invariable label\n",
    "\n",
    "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
    "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AASHISH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\AASHISH\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# input with shape of height=32 and width=128 \n",
    "inputs = Input(shape=(32,128,1))\n",
    " \n",
    "# convolution layer with kernel size (3,3)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs) #convolution layer 1\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)# poolig layer with kernel size (2,2)\n",
    " \n",
    "conv_2 = Conv2D(64, (3,3), activation = 'relu', padding='same')(pool_1)#convolution layer 2\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    " \n",
    "conv_3 = Conv2D(64, (3,3), activation = 'relu', padding='same')(pool_2)#convolution layer 3\n",
    "batch_norm_1 = BatchNormalization()(conv_3)\n",
    " \n",
    "conv_4 = Conv2D(64, (3,3), activation = 'relu', padding='same')(batch_norm_1)#convolution layer 4\n",
    "batch_norm_2 = BatchNormalization()(conv_4)# Batch normalization layer\n",
    " \n",
    "conv_5 = Conv2D(64, (3,3), activation = 'relu', padding='same')(batch_norm_2)#convolution layer 5\n",
    "batch_norm_3 = BatchNormalization()(conv_5)# Batch normalization layer\n",
    "pool_3 = MaxPool2D(pool_size=(3, 1))(batch_norm_3)\n",
    " \n",
    "conv_6 = Conv2D(64, (2,2), activation = 'relu')(pool_3)#convolution layer 6\n",
    " \n",
    "squeezed_layer = Lambda(lambda x: K.squeeze(x, 1))(conv_6)\n",
    " \n",
    "# bidirectional LSTM layers with units = 128 and 64\n",
    "blstm_layer_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed_layer)\n",
    "blstm_layer_2 = Bidirectional(LSTM(64, return_sequences=True,dropout = 0.2))(blstm_layer_1)\n",
    " \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_layer_2)\n",
    "\n",
    "act_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 128, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 128, 64)       640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 32, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 32, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 32, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 32, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 32, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 32, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 32, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 32, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 31, 64)         16448     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 31, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 31, 256)           197632    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 31, 128)           164352    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31, 63)            8127      \n",
      "=================================================================\n",
      "Total params: 535,679\n",
      "Trainable params: 535,295\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "act_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AASHISH\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\AASHISH\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defining Loss function\n",
    "\n",
    "I have used CTC loss funtion for recognizing license plate. CTC overcome the problem of sequence generation.\n",
    "Secondly, It take care of multiple time steps. for example, word is texture \n",
    "and RNN model predits ['t','e','e','x','t','t','u','r','e'].so, to predict the output, we need to merge the character\n",
    "adjacent to each other. But this can also predict the word ['text'] which is wrong.so CTC take care of this things.Basically,\n",
    "it place blank between the predicted output ['t','e','e','x','-','t','t','u','r','e'] and then merge and can predict correctly.\n",
    "\n",
    "\n",
    "So CTC take 4 inputs:\n",
    "\n",
    "1. y_true: your ground truth data. The data you are going to compare with the model's outputs in training. \n",
    "2. y_pred: is the model's calculated output\n",
    "3. input_length:the length (in steps, or chars) of each sample (sentence) in the y_pred tensor.\n",
    "4. label_length:the length (in steps, or chars ) of each sample (sentence) in the y_true (or labels) tensor.\n",
    "\n",
    "''' \n",
    "\n",
    "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    " \n",
    "def ctc_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    " \n",
    "loss_out = Lambda(ctc_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
    "\n",
    "#model to be used at training time\n",
    "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now i compile the model with RMSPROP optimizer with learning_rate = 0.001, and save the weights in \"my_model.h5\" on the \n",
    "basis of validation loss.\n",
    "\n",
    "'''\n",
    "\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = optimizers.RMSprop(lr=0.001))\n",
    " \n",
    "filepath=\"my_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_img = np.array(training_img)\n",
    "train_input_length = np.array(train_input_length)\n",
    "train_label_length = np.array(train_label_length)\n",
    "\n",
    "valid_img = np.array(valid_img)\n",
    "valid_input_length = np.array(valid_input_length)\n",
    "valid_label_length = np.array(valid_label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_img=training_img.reshape(521, 32, 128,1)\n",
    "valid_img = valid_img.reshape(131, 32, 128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AASHISH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 521 samples, validate on 131 samples\n",
      "Epoch 1/100\n",
      "521/521 [==============================] - 21s 40ms/step - loss: 25.9623 - val_loss: 20.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 20.36287, saving model to keras_model.pkl\n",
      "Epoch 2/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 19.2434 - val_loss: 29.0273\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 20.36287\n",
      "Epoch 3/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 18.1530 - val_loss: 18.6307\n",
      "\n",
      "Epoch 00003: val_loss improved from 20.36287 to 18.63070, saving model to keras_model.pkl\n",
      "Epoch 4/100\n",
      "521/521 [==============================] - 15s 28ms/step - loss: 17.6780 - val_loss: 22.7091\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 18.63070\n",
      "Epoch 5/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 17.5434 - val_loss: 17.7970\n",
      "\n",
      "Epoch 00005: val_loss improved from 18.63070 to 17.79695, saving model to keras_model.pkl\n",
      "Epoch 6/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 17.5036 - val_loss: 17.8383\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 17.79695\n",
      "Epoch 7/100\n",
      "521/521 [==============================] - 19s 36ms/step - loss: 17.2094 - val_loss: 18.1695\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 17.79695\n",
      "Epoch 8/100\n",
      "521/521 [==============================] - 16s 32ms/step - loss: 17.1545 - val_loss: 17.4888\n",
      "\n",
      "Epoch 00008: val_loss improved from 17.79695 to 17.48877, saving model to keras_model.pkl\n",
      "Epoch 9/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 17.0213 - val_loss: 17.6668\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 17.48877\n",
      "Epoch 10/100\n",
      "521/521 [==============================] - 16s 32ms/step - loss: 16.8325 - val_loss: 17.6297\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 17.48877\n",
      "Epoch 11/100\n",
      "521/521 [==============================] - 16s 32ms/step - loss: 16.7149 - val_loss: 17.4430\n",
      "\n",
      "Epoch 00011: val_loss improved from 17.48877 to 17.44304, saving model to keras_model.pkl\n",
      "Epoch 12/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 16.5456 - val_loss: 17.8238\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 17.44304\n",
      "Epoch 13/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 16.3477 - val_loss: 17.7661\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 17.44304\n",
      "Epoch 14/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 16.1022 - val_loss: 17.5950\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 17.44304\n",
      "Epoch 15/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 15.9310 - val_loss: 17.2134\n",
      "\n",
      "Epoch 00015: val_loss improved from 17.44304 to 17.21338, saving model to keras_model.pkl\n",
      "Epoch 16/100\n",
      "521/521 [==============================] - 15s 30ms/step - loss: 15.6553 - val_loss: 17.4270\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 17.21338\n",
      "Epoch 17/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 15.4806 - val_loss: 16.6304\n",
      "\n",
      "Epoch 00017: val_loss improved from 17.21338 to 16.63043, saving model to keras_model.pkl\n",
      "Epoch 18/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 15.0779 - val_loss: 16.8405\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 16.63043\n",
      "Epoch 19/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 14.9025 - val_loss: 16.2361\n",
      "\n",
      "Epoch 00019: val_loss improved from 16.63043 to 16.23610, saving model to keras_model.pkl\n",
      "Epoch 20/100\n",
      "521/521 [==============================] - 18s 35ms/step - loss: 14.5348 - val_loss: 17.0315\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 16.23610\n",
      "Epoch 21/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 14.2610 - val_loss: 16.0534\n",
      "\n",
      "Epoch 00021: val_loss improved from 16.23610 to 16.05337, saving model to keras_model.pkl\n",
      "Epoch 22/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 13.8497 - val_loss: 16.2192\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 16.05337\n",
      "Epoch 23/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 13.5291 - val_loss: 15.5241\n",
      "\n",
      "Epoch 00023: val_loss improved from 16.05337 to 15.52413, saving model to keras_model.pkl\n",
      "Epoch 24/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 13.0887 - val_loss: 14.3932\n",
      "\n",
      "Epoch 00024: val_loss improved from 15.52413 to 14.39321, saving model to keras_model.pkl\n",
      "Epoch 25/100\n",
      "521/521 [==============================] - 15s 28ms/step - loss: 12.4441 - val_loss: 14.1318\n",
      "\n",
      "Epoch 00025: val_loss improved from 14.39321 to 14.13181, saving model to keras_model.pkl\n",
      "Epoch 26/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 12.0935 - val_loss: 14.6801\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 14.13181\n",
      "Epoch 27/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 11.6017 - val_loss: 14.0444\n",
      "\n",
      "Epoch 00027: val_loss improved from 14.13181 to 14.04442, saving model to keras_model.pkl\n",
      "Epoch 28/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 10.9999 - val_loss: 14.1867\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 14.04442\n",
      "Epoch 29/100\n",
      "521/521 [==============================] - 15s 28ms/step - loss: 10.5459 - val_loss: 12.7658\n",
      "\n",
      "Epoch 00029: val_loss improved from 14.04442 to 12.76577, saving model to keras_model.pkl\n",
      "Epoch 30/100\n",
      "521/521 [==============================] - 18s 35ms/step - loss: 9.8195 - val_loss: 11.6928\n",
      "\n",
      "Epoch 00030: val_loss improved from 12.76577 to 11.69284, saving model to keras_model.pkl\n",
      "Epoch 31/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 9.1536 - val_loss: 11.9743\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 11.69284\n",
      "Epoch 32/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 8.6043 - val_loss: 10.9910\n",
      "\n",
      "Epoch 00032: val_loss improved from 11.69284 to 10.99101, saving model to keras_model.pkl\n",
      "Epoch 33/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 7.8257 - val_loss: 10.3396\n",
      "\n",
      "Epoch 00033: val_loss improved from 10.99101 to 10.33958, saving model to keras_model.pkl\n",
      "Epoch 34/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 7.2512 - val_loss: 12.0369\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 10.33958\n",
      "Epoch 35/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 6.6429 - val_loss: 9.4037\n",
      "\n",
      "Epoch 00035: val_loss improved from 10.33958 to 9.40365, saving model to keras_model.pkl\n",
      "Epoch 36/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 5.9235 - val_loss: 7.9933\n",
      "\n",
      "Epoch 00036: val_loss improved from 9.40365 to 7.99328, saving model to keras_model.pkl\n",
      "Epoch 37/100\n",
      "521/521 [==============================] - 16s 32ms/step - loss: 5.3120 - val_loss: 7.2638\n",
      "\n",
      "Epoch 00037: val_loss improved from 7.99328 to 7.26385, saving model to keras_model.pkl\n",
      "Epoch 38/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 4.9861 - val_loss: 8.2737\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 7.26385\n",
      "Epoch 39/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 4.4627 - val_loss: 6.2105\n",
      "\n",
      "Epoch 00039: val_loss improved from 7.26385 to 6.21054, saving model to keras_model.pkl\n",
      "Epoch 40/100\n",
      "521/521 [==============================] - 19s 36ms/step - loss: 3.9178 - val_loss: 5.8942\n",
      "\n",
      "Epoch 00040: val_loss improved from 6.21054 to 5.89420, saving model to keras_model.pkl\n",
      "Epoch 41/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 3.7114 - val_loss: 4.8277\n",
      "\n",
      "Epoch 00041: val_loss improved from 5.89420 to 4.82765, saving model to keras_model.pkl\n",
      "Epoch 42/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 3.2762 - val_loss: 5.1603\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 4.82765\n",
      "Epoch 43/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 3.0492 - val_loss: 4.9451\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 4.82765\n",
      "Epoch 44/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 2.6134 - val_loss: 3.4871\n",
      "\n",
      "Epoch 00044: val_loss improved from 4.82765 to 3.48714, saving model to keras_model.pkl\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521/521 [==============================] - 17s 33ms/step - loss: 2.4086 - val_loss: 3.1479\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.48714 to 3.14787, saving model to keras_model.pkl\n",
      "Epoch 46/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 2.0645 - val_loss: 4.9499\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 3.14787\n",
      "Epoch 47/100\n",
      "521/521 [==============================] - 16s 32ms/step - loss: 2.0587 - val_loss: 2.4299\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.14787 to 2.42995, saving model to keras_model.pkl\n",
      "Epoch 48/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 2.0200 - val_loss: 2.0445\n",
      "\n",
      "Epoch 00048: val_loss improved from 2.42995 to 2.04450, saving model to keras_model.pkl\n",
      "Epoch 49/100\n",
      "521/521 [==============================] - 18s 35ms/step - loss: 1.6323 - val_loss: 2.1368\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.04450\n",
      "Epoch 50/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 1.6363 - val_loss: 2.1188\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.04450\n",
      "Epoch 51/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 1.5984 - val_loss: 2.1333\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.04450\n",
      "Epoch 52/100\n",
      "521/521 [==============================] - 19s 36ms/step - loss: 1.3030 - val_loss: 2.4554\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.04450\n",
      "Epoch 53/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 1.3693 - val_loss: 5.6312\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.04450\n",
      "Epoch 54/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 1.2633 - val_loss: 2.7085\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.04450\n",
      "Epoch 55/100\n",
      "521/521 [==============================] - 19s 37ms/step - loss: 1.0425 - val_loss: 3.9619\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.04450\n",
      "Epoch 56/100\n",
      "521/521 [==============================] - 19s 37ms/step - loss: 1.0229 - val_loss: 1.2284\n",
      "\n",
      "Epoch 00056: val_loss improved from 2.04450 to 1.22841, saving model to keras_model.pkl\n",
      "Epoch 57/100\n",
      "521/521 [==============================] - 18s 35ms/step - loss: 0.9327 - val_loss: 1.8791\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.22841\n",
      "Epoch 58/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 1.0309 - val_loss: 1.3331\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.22841\n",
      "Epoch 59/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 0.9750 - val_loss: 1.5333\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.22841\n",
      "Epoch 60/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 0.8413 - val_loss: 1.1176\n",
      "\n",
      "Epoch 00060: val_loss improved from 1.22841 to 1.11760, saving model to keras_model.pkl\n",
      "Epoch 61/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 0.7550 - val_loss: 0.9509\n",
      "\n",
      "Epoch 00061: val_loss improved from 1.11760 to 0.95090, saving model to keras_model.pkl\n",
      "Epoch 62/100\n",
      "521/521 [==============================] - 18s 35ms/step - loss: 0.6206 - val_loss: 1.1017\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.95090\n",
      "Epoch 63/100\n",
      "521/521 [==============================] - 15s 28ms/step - loss: 0.8170 - val_loss: 2.1082\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.95090\n",
      "Epoch 64/100\n",
      "521/521 [==============================] - 19s 36ms/step - loss: 0.7645 - val_loss: 1.1989\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.95090\n",
      "Epoch 65/100\n",
      "521/521 [==============================] - 18s 35ms/step - loss: 0.6444 - val_loss: 1.8632\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.95090\n",
      "Epoch 66/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 0.6798 - val_loss: 1.5860\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.95090\n",
      "Epoch 67/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 0.6540 - val_loss: 1.2947\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.95090\n",
      "Epoch 68/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.6077 - val_loss: 2.7617\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.95090\n",
      "Epoch 69/100\n",
      "521/521 [==============================] - 15s 28ms/step - loss: 0.6787 - val_loss: 0.3643\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.95090 to 0.36425, saving model to keras_model.pkl\n",
      "Epoch 70/100\n",
      "521/521 [==============================] - 15s 28ms/step - loss: 0.5626 - val_loss: 1.7352\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.36425\n",
      "Epoch 71/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.5666 - val_loss: 0.7742\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.36425\n",
      "Epoch 72/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.5656 - val_loss: 1.0039\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.36425\n",
      "Epoch 73/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 0.6060 - val_loss: 1.2573\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.36425\n",
      "Epoch 74/100\n",
      "521/521 [==============================] - 18s 34ms/step - loss: 0.3699 - val_loss: 0.3844\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.36425\n",
      "Epoch 75/100\n",
      "521/521 [==============================] - 20s 39ms/step - loss: 0.5798 - val_loss: 0.8866\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.36425\n",
      "Epoch 76/100\n",
      "521/521 [==============================] - 24s 47ms/step - loss: 0.5441 - val_loss: 1.1095\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.36425\n",
      "Epoch 77/100\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.4935 - val_loss: 0.5866\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.36425\n",
      "Epoch 78/100\n",
      "521/521 [==============================] - 20s 38ms/step - loss: 0.4784 - val_loss: 0.4920\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.36425\n",
      "Epoch 79/100\n",
      "521/521 [==============================] - 21s 41ms/step - loss: 0.4304 - val_loss: 0.7162\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.36425\n",
      "Epoch 80/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 0.4274 - val_loss: 0.7267\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.36425\n",
      "Epoch 81/100\n",
      "521/521 [==============================] - 15s 30ms/step - loss: 0.4632 - val_loss: 0.2684\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.36425 to 0.26844, saving model to keras_model.pkl\n",
      "Epoch 82/100\n",
      "521/521 [==============================] - 19s 36ms/step - loss: 0.4007 - val_loss: 0.8866\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.26844\n",
      "Epoch 83/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.4073 - val_loss: 0.4550\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.26844\n",
      "Epoch 84/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.3911 - val_loss: 1.3081\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.26844\n",
      "Epoch 85/100\n",
      "521/521 [==============================] - 17s 33ms/step - loss: 0.4727 - val_loss: 0.2157\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.26844 to 0.21569, saving model to keras_model.pkl\n",
      "Epoch 86/100\n",
      "521/521 [==============================] - 17s 32ms/step - loss: 0.3897 - val_loss: 2.1608\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.21569\n",
      "Epoch 87/100\n",
      "521/521 [==============================] - 19s 36ms/step - loss: 0.4524 - val_loss: 0.6998\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.21569\n",
      "Epoch 88/100\n",
      "521/521 [==============================] - 20s 38ms/step - loss: 0.4249 - val_loss: 0.7691\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.21569\n",
      "Epoch 89/100\n",
      "521/521 [==============================] - 16s 32ms/step - loss: 0.2991 - val_loss: 0.1522\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.21569 to 0.15220, saving model to keras_model.pkl\n",
      "Epoch 90/100\n",
      "521/521 [==============================] - 14s 28ms/step - loss: 0.4442 - val_loss: 0.4804\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.15220\n",
      "Epoch 91/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 0.2720 - val_loss: 0.2047\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.15220\n",
      "Epoch 92/100\n",
      "521/521 [==============================] - 15s 28ms/step - loss: 0.3821 - val_loss: 0.1177\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.15220 to 0.11774, saving model to keras_model.pkl\n",
      "Epoch 93/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.2696 - val_loss: 1.0467\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.11774\n",
      "Epoch 94/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.3342 - val_loss: 0.4919\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.11774\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521/521 [==============================] - 15s 29ms/step - loss: 0.3123 - val_loss: 0.2111\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.11774\n",
      "Epoch 96/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 0.3967 - val_loss: 0.4874\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.11774\n",
      "Epoch 97/100\n",
      "521/521 [==============================] - 15s 30ms/step - loss: 0.3167 - val_loss: 0.4946\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.11774\n",
      "Epoch 98/100\n",
      "521/521 [==============================] - 16s 30ms/step - loss: 0.4184 - val_loss: 0.0674\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.11774 to 0.06741, saving model to keras_model.pkl\n",
      "Epoch 99/100\n",
      "521/521 [==============================] - 16s 31ms/step - loss: 0.2372 - val_loss: 0.4926\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.06741\n",
      "Epoch 100/100\n",
      "521/521 [==============================] - 15s 29ms/step - loss: 0.3386 - val_loss: 0.3001\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.06741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29a84a806d8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_model.load_weights('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_text =  1B85815\n",
      "predicted text = 1B85815\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "In CTC decoder, it removes the duplicates character (inserting blank) and also take the highest probability \n",
    "of the character in each step.\n",
    "\n",
    "'''\n",
    "\n",
    "# load the saved best model weights\n",
    "# act_model = joblib.load('keras_model.pkl')  \n",
    "# act_model = load_model('keras_model.pkl')\n",
    "# act_model = load_model('keras_model.pkl', compile=False)\n",
    "\n",
    "act_model.load_weights('my_model.h5')\n",
    "im = '1B85815.jpg'   #path of test image\n",
    "\n",
    "img = cv2.cvtColor(cv2.imread(im), cv2.COLOR_BGR2GRAY)\n",
    "img = cv2.resize(img, (128,32)) #resizing\n",
    "plt.imshow(img)\n",
    "plt.show\n",
    "\n",
    "prediction = act_model.predict(img.reshape(1,32, 128, 1)) #reshaping the img to pass in model\n",
    " \n",
    "# use CTC decoder\n",
    "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
    "                         greedy=False)[0][0])\n",
    " \n",
    "# see the results\n",
    "\n",
    "for x in out:\n",
    "    print(\"original_text = \",im.split('.')[0])\n",
    "    print(\"predicted text = \", end = '')\n",
    "    for p in x:\n",
    "        if int(p) != -1:\n",
    "            print(char_list[int(p)], end = '')       \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
